{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7dbbad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 16:24:05.547152: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-06 16:24:05.573324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 16:24:05.942339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv2D, Reshape, Activation, Flatten\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59627db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_dir = '/home/jeffreydhy/data/projects/ares-finance/generated/images/labeled/AMD_30min_segments'\n",
    "images_dir = '/home/jeffreydhy/data/projects/ares-finance/generated/images/unlabeled/AMD_30min_segments'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa3153",
   "metadata": {},
   "source": [
    "## Batch Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df308b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmls_to_dataframe(xmls_dir, images_dir):\n",
    "    data = []\n",
    "    for xml_filename in os.listdir(xmls_dir):\n",
    "        # Read the XML file\n",
    "        xml_path = os.path.join(xmls_dir, xml_filename)\n",
    "        tree = ElementTree.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        bndbox = root.find(\".//bndbox\")\n",
    "        xmin = float(bndbox.find(\"xmin\").text)\n",
    "        ymin = float(bndbox.find(\"ymin\").text)\n",
    "        xmax = float(bndbox.find(\"xmax\").text)\n",
    "        ymax = float(bndbox.find(\"ymax\").text)\n",
    "        size = root.find(\".//size\")\n",
    "        height = float(size.find(\"height\").text)\n",
    "        width = float(size.find(\"width\").text)\n",
    "\n",
    "        # Add the image file path and bounding box coordinates to the DataFrame\n",
    "        image_filename = os.path.splitext(xml_filename)[0] + \".png\"\n",
    "        image_path = os.path.join(images_dir, image_filename)\n",
    "        data.append([image_path, xmin, ymin, xmax, ymax, height, width])\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"image_path\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"height\", \"width\"])\n",
    "\n",
    "# Normalize Bounding Boxes\n",
    "def preprocess_bounding_boxes(data_df, input_shape):\n",
    "    img_height, img_width, _ = input_shape\n",
    "\n",
    "    for idx, row in data_df.iterrows():\n",
    "        # Read original image dimensions\n",
    "        original_height, original_width = row[\"height\"], row[\"width\"]\n",
    "\n",
    "        # Calculate scaling factors\n",
    "        width_scale = img_width / original_width\n",
    "        height_scale = img_height / original_height\n",
    "\n",
    "        # Scale and normalize bounding box coordinates\n",
    "        xmin = row[\"xmin\"] * width_scale\n",
    "        ymin = row[\"ymin\"] * height_scale\n",
    "        xmax = row[\"xmax\"] * width_scale\n",
    "        ymax = row[\"ymax\"] * height_scale\n",
    "\n",
    "        # Update the dataframe with the new coordinates\n",
    "        data_df.at[idx, \"xmin\"] = xmin / img_width\n",
    "        data_df.at[idx, \"ymin\"] = ymin / img_height\n",
    "        data_df.at[idx, \"xmax\"] = xmax / img_width\n",
    "        data_df.at[idx, \"ymax\"] = ymax / img_height\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa32490",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = xmls_to_dataframe(xmls_dir, images_dir)\n",
    "batch_size = 16\n",
    "input_shape = (224, 224, 3)\n",
    "data_df = preprocess_bounding_boxes(data_df, input_shape)\n",
    "data_df['confidence'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444273e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa559343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116 validated image filenames.\n",
      "Found 30 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Create the ImageDataGenerator with data augmentation options\n",
    "image_data_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=False)\n",
    "\n",
    "\n",
    "train_generator = image_data_gen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=None,\n",
    "    x_col='image_path',\n",
    "    y_col=['xmin', 'ymin', 'xmax', 'ymax', 'confidence'],\n",
    "    target_size=input_shape[:2],\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "val_generator = image_data_gen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=None,\n",
    "    x_col='image_path',\n",
    "    y_col=['xmin', 'ymin', 'xmax', 'ymax', 'confidence'],\n",
    "    target_size=input_shape[:2],\n",
    "    color_mode='rgb',\n",
    "    class_mode='raw',\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24fd8d",
   "metadata": {},
   "source": [
    "### Visualize the data with bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afba569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_bbox(image, bboxes):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    img_height, img_width, _ = image.shape\n",
    "\n",
    "    for box in bboxes:\n",
    "        x, y, x_max, y_max = box\n",
    "        x *= img_width\n",
    "        y *= img_height\n",
    "        x_max *= img_width\n",
    "        y_max *= img_height\n",
    "\n",
    "        width = x_max - x\n",
    "        height = y_max - y\n",
    "\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# images_batch, bboxes_batch = next(train_generator)\n",
    "# image_index = 0\n",
    "# image = images_batch[image_index]\n",
    "# bboxes = [bboxes_batch[image_index]]\n",
    "\n",
    "# print(\"bboxes_batch shape:\", bboxes_batch.shape)\n",
    "# print(\"bboxes:\", bboxes)\n",
    "\n",
    "# visualize_image_bbox(image, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b2604",
   "metadata": {},
   "source": [
    "### Inspect GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462b591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "# print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     print(\"GPU:\", gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88ab443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 16:24:14.305821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.342059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.342191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.343318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.343425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.343499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.637757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.637887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.637971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-06 16:24:14.638046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22072 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:03:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, None, None,   0           ['input_1[0][0]']                \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, None, None,   9472        ['conv1_pad[0][0]']              \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, None, None,   256         ['conv1_conv[0][0]']             \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, None, None,   0           ['conv1_bn[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, None, None,   0           ['conv1_relu[0][0]']             \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, None, None,   0           ['pool1_pad[0][0]']              \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, None, None,   4160        ['pool1_pool[0][0]']             \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, None, None,   256        ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                       64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, None, None,   0          ['conv2_block1_1_bn[0][0]']      \n",
      " n)                             64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, None, None,   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, None, None,   256        ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                       64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, None, None,   0          ['conv2_block1_2_bn[0][0]']      \n",
      " n)                             64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, None, None,   16640       ['pool1_pool[0][0]']             \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, None, None,   16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, None, None,   1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, None, None,   1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, None, None,   0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                256)                              'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, None, None,   0           ['conv2_block1_add[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, None, None,   16448       ['conv2_block1_out[0][0]']       \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, None, None,   256        ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                       64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, None, None,   0          ['conv2_block2_1_bn[0][0]']      \n",
      " n)                             64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, None, None,   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, None, None,   256        ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                       64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, None, None,   0          ['conv2_block2_2_bn[0][0]']      \n",
      " n)                             64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, None, None,   16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, None, None,   1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                       256)                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, None, None,   0           ['conv2_block1_out[0][0]',       \n",
      "                                256)                              'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, None, None,   0           ['conv2_block2_add[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, None, None,   16448       ['conv2_block2_out[0][0]']       \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, None, None,   256        ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                       64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, None, None,   0          ['conv2_block3_1_bn[0][0]']      \n",
      " n)                             64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, None, None,   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, None, None,   256        ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                       64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, None, None,   0          ['conv2_block3_2_bn[0][0]']      \n",
      " n)                             64)                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, None, None,   16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, None, None,   1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, None, None,   0           ['conv2_block2_out[0][0]',       \n",
      "                                256)                              'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, None, None,   0           ['conv2_block3_add[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, None, None,   32896       ['conv2_block3_out[0][0]']       \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, None, None,   512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, None, None,   0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, None, None,   147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, None, None,   512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, None, None,   0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, None, None,   131584      ['conv2_block3_out[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, None, None,   66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, None, None,   2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, None, None,   2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, None, None,   0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                512)                              'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, None, None,   0           ['conv3_block1_add[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, None, None,   65664       ['conv3_block1_out[0][0]']       \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, None, None,   512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, None, None,   0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, None, None,   147584      ['conv3_block2_1_relu[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, None, None,   512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, None, None,   0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, None, None,   66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, None, None,   2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, None, None,   0           ['conv3_block1_out[0][0]',       \n",
      "                                512)                              'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, None, None,   0           ['conv3_block2_add[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, None, None,   65664       ['conv3_block2_out[0][0]']       \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, None, None,   512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, None, None,   0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, None, None,   147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, None, None,   512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, None, None,   0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, None, None,   66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, None, None,   2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, None, None,   0           ['conv3_block2_out[0][0]',       \n",
      "                                512)                              'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, None, None,   0           ['conv3_block3_add[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, None, None,   65664       ['conv3_block3_out[0][0]']       \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, None, None,   512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, None, None,   0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, None, None,   147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, None, None,   512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                       128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, None, None,   0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, None, None,   66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, None, None,   2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, None, None,   0           ['conv3_block3_out[0][0]',       \n",
      "                                512)                              'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, None, None,   0           ['conv3_block4_add[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, None, None,   131328      ['conv3_block4_out[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block1_1_bn (BatchNormal  (None, None, None,   1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, None, None,   0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, None, None,   590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, None, None,   1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, None, None,   0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, None, None,   525312      ['conv3_block4_out[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, None, None,   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, None, None,   4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, None, None,   4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, None, None,   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                1024)                             'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, None, None,   0           ['conv4_block1_add[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, None, None,   262400      ['conv4_block1_out[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, None, None,   1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, None, None,   0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, None, None,   590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, None, None,   1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, None, None,   0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, None, None,   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, None, None,   4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, None, None,   0           ['conv4_block1_out[0][0]',       \n",
      "                                1024)                             'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, None, None,   0           ['conv4_block2_add[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, None, None,   262400      ['conv4_block2_out[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, None, None,   1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, None, None,   0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, None, None,   590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, None, None,   1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, None, None,   0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, None, None,   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                1024)                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, None, None,   4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, None, None,   0           ['conv4_block2_out[0][0]',       \n",
      "                                1024)                             'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, None, None,   0           ['conv4_block3_add[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, None, None,   262400      ['conv4_block3_out[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, None, None,   1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, None, None,   0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, None, None,   590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, None, None,   1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, None, None,   0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, None, None,   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, None, None,   4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, None, None,   0           ['conv4_block3_out[0][0]',       \n",
      "                                1024)                             'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, None, None,   0           ['conv4_block4_add[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, None, None,   262400      ['conv4_block4_out[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, None, None,   1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, None, None,   0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, None, None,   590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, None, None,   1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, None, None,   0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, None, None,   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, None, None,   4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, None, None,   0           ['conv4_block4_out[0][0]',       \n",
      "                                1024)                             'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, None, None,   0           ['conv4_block5_add[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, None, None,   262400      ['conv4_block5_out[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, None, None,   1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, None, None,   0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, None, None,   590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, None, None,   1024       ['conv4_block6_2_conv[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ization)                       256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, None, None,   0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, None, None,   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, None, None,   4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       1024)                                                             \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, None, None,   0           ['conv4_block5_out[0][0]',       \n",
      "                                1024)                             'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, None, None,   0           ['conv4_block6_add[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, None, None,   524800      ['conv4_block6_out[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, None, None,   2048       ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, None, None,   0          ['conv5_block1_1_bn[0][0]']      \n",
      " n)                             512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, None, None,   2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, None, None,   2048       ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, None, None,   0          ['conv5_block1_2_bn[0][0]']      \n",
      " n)                             512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, None, None,   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, None, None,   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, None, None,   8192       ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                       2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, None, None,   8192       ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                       2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, None, None,   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                2048)                             'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, None, None,   0           ['conv5_block1_add[0][0]']       \n",
      "                                2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, None, None,   1049088     ['conv5_block1_out[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, None, None,   2048       ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, None, None,   0          ['conv5_block2_1_bn[0][0]']      \n",
      " n)                             512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, None, None,   2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, None, None,   2048       ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, None, None,   0          ['conv5_block2_2_bn[0][0]']      \n",
      " n)                             512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, None, None,   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, None, None,   8192       ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                       2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, None, None,   0           ['conv5_block1_out[0][0]',       \n",
      "                                2048)                             'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, None, None,   0           ['conv5_block2_add[0][0]']       \n",
      "                                2048)                                                             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv5_block3_1_conv (Conv2D)   (None, None, None,   1049088     ['conv5_block2_out[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, None, None,   2048       ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, None, None,   0          ['conv5_block3_1_bn[0][0]']      \n",
      " n)                             512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, None, None,   2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, None, None,   2048       ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                       512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, None, None,   0          ['conv5_block3_2_bn[0][0]']      \n",
      " n)                             512)                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, None, None,   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, None, None,   8192       ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                       2048)                                                             \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, None, None,   0           ['conv5_block2_out[0][0]',       \n",
      "                                2048)                             'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, None, None,   0           ['conv5_block3_add[0][0]']       \n",
      "                                2048)                                                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            10245       ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,597,957\n",
      "Trainable params: 23,544,837\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 1\n",
    "\n",
    "# Initialize the MirroredStrategy for multi-GPU training\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global average pooling layer followed by a dense layer with 4 output units\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Dense(5, activation='linear')(x)\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Calculate mean squared error for bounding box coordinates\n",
    "    mse = K.mean(K.square(y_true[:, :4] - y_pred[:, :4]), axis=-1)\n",
    "\n",
    "    # Calculate binary cross-entropy for the confidence score\n",
    "    bce = K.binary_crossentropy(y_true[:, 4], K.sigmoid(y_pred[:, 4]))\n",
    "\n",
    "    # Combine the losses\n",
    "    return mse + bce\n",
    "\n",
    "# Compile the model with Mean Squared Error loss and the Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=custom_loss)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10549609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 224, 224, 3) (16, 5)\n"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch = len(train_df) // batch_size\n",
    "val_steps_per_epoch = len(val_df) // batch_size\n",
    "\n",
    "x, y = next(train_generator)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da863a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 16:24:27.340073: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-06 16:24:32.577576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-06 16:24:33.114291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-06 16:24:33.115944: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x564a6779a370 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-06 16:24:33.115963: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-05-06 16:24:33.119105: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-06 16:24:33.198419: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 0s - loss: 0.3613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 16:24:43.441704: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.21566, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 17s 335ms/step - loss: 0.3295 - val_loss: 2.2157\n",
      "Epoch 2/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1455\n",
      "Epoch 2: val_loss did not improve from 2.21566\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.1455 - val_loss: 2.3137\n",
      "Epoch 3/300\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0721\n",
      "Epoch 3: val_loss improved from 2.21566 to 1.88180, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.0690 - val_loss: 1.8818\n",
      "Epoch 4/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0468\n",
      "Epoch 4: val_loss improved from 1.88180 to 1.55109, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 0.0468 - val_loss: 1.5511\n",
      "Epoch 5/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0459\n",
      "Epoch 5: val_loss improved from 1.55109 to 1.16759, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 0.0459 - val_loss: 1.1676\n",
      "Epoch 6/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0386\n",
      "Epoch 6: val_loss improved from 1.16759 to 0.76166, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 0.0386 - val_loss: 0.7617\n",
      "Epoch 7/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0301\n",
      "Epoch 7: val_loss improved from 0.76166 to 0.66714, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 0.0301 - val_loss: 0.6671\n",
      "Epoch 8/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0207\n",
      "Epoch 8: val_loss improved from 0.66714 to 0.56044, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.0207 - val_loss: 0.5604\n",
      "Epoch 9/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0180\n",
      "Epoch 9: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0180 - val_loss: 0.5737\n",
      "Epoch 10/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0188\n",
      "Epoch 10: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0188 - val_loss: 0.6486\n",
      "Epoch 11/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0105\n",
      "Epoch 11: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0105 - val_loss: 0.6410\n",
      "Epoch 12/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0089\n",
      "Epoch 12: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0089 - val_loss: 0.6544\n",
      "Epoch 13/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0129\n",
      "Epoch 13: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0139 - val_loss: 0.7249\n",
      "Epoch 14/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0095\n",
      "Epoch 14: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0098 - val_loss: 0.7798\n",
      "Epoch 15/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0086\n",
      "Epoch 15: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0086 - val_loss: 0.7654\n",
      "Epoch 16/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0122\n",
      "Epoch 16: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0122 - val_loss: 0.7896\n",
      "Epoch 17/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0124\n",
      "Epoch 17: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0124 - val_loss: 0.7860\n",
      "Epoch 18/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0104\n",
      "Epoch 18: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0104 - val_loss: 0.8232\n",
      "Epoch 19/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0090\n",
      "Epoch 19: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0090 - val_loss: 0.8777\n",
      "Epoch 20/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0157\n",
      "Epoch 20: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0150 - val_loss: 0.8869\n",
      "Epoch 21/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0148\n",
      "Epoch 21: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0143 - val_loss: 0.8791\n",
      "Epoch 22/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0123\n",
      "Epoch 22: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0123 - val_loss: 0.9278\n",
      "Epoch 23/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0110\n",
      "Epoch 23: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0110 - val_loss: 0.9976\n",
      "Epoch 24/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0094\n",
      "Epoch 24: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0094 - val_loss: 1.0333\n",
      "Epoch 25/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 25: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 0.0072 - val_loss: 1.0763\n",
      "Epoch 26/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0054\n",
      "Epoch 26: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0054 - val_loss: 1.1049\n",
      "Epoch 27/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0098\n",
      "Epoch 27: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.0098 - val_loss: 1.1520\n",
      "Epoch 28/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0101\n",
      "Epoch 28: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0101 - val_loss: 1.2694\n",
      "Epoch 29/300\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0081\n",
      "Epoch 29: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0076 - val_loss: 1.3354\n",
      "Epoch 30/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0119\n",
      "Epoch 30: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 0.0119 - val_loss: 1.3161\n",
      "Epoch 31/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0104\n",
      "Epoch 31: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0104 - val_loss: 1.3170\n",
      "Epoch 32/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0109\n",
      "Epoch 32: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.0109 - val_loss: 1.3569\n",
      "Epoch 33/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0128\n",
      "Epoch 33: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0128 - val_loss: 1.4066\n",
      "Epoch 34/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0131\n",
      "Epoch 34: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0131 - val_loss: 1.4458\n",
      "Epoch 35/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0189\n",
      "Epoch 35: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0189 - val_loss: 1.4508\n",
      "Epoch 36/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 36: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0168 - val_loss: 1.4405\n",
      "Epoch 37/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0212\n",
      "Epoch 37: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0212 - val_loss: 1.3642\n",
      "Epoch 38/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0278\n",
      "Epoch 38: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0275 - val_loss: 1.3518\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0171\n",
      "Epoch 39: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0171 - val_loss: 1.3487\n",
      "Epoch 40/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0126\n",
      "Epoch 40: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0126 - val_loss: 1.3353\n",
      "Epoch 41/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0103\n",
      "Epoch 41: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0103 - val_loss: 1.3680\n",
      "Epoch 42/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0084\n",
      "Epoch 42: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0084 - val_loss: 1.3413\n",
      "Epoch 43/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0091\n",
      "Epoch 43: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0091 - val_loss: 1.3776\n",
      "Epoch 44/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0150\n",
      "Epoch 44: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0150 - val_loss: 1.3816\n",
      "Epoch 45/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0146\n",
      "Epoch 45: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0146 - val_loss: 1.3258\n",
      "Epoch 46/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0098\n",
      "Epoch 46: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0098 - val_loss: 1.3408\n",
      "Epoch 47/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0069\n",
      "Epoch 47: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.0069 - val_loss: 1.2936\n",
      "Epoch 48/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0084\n",
      "Epoch 48: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0084 - val_loss: 1.2649\n",
      "Epoch 49/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0080\n",
      "Epoch 49: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0080 - val_loss: 1.2251\n",
      "Epoch 50/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0054\n",
      "Epoch 50: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0054 - val_loss: 1.2193\n",
      "Epoch 51/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 51: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0058 - val_loss: 1.2128\n",
      "Epoch 52/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0080\n",
      "Epoch 52: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0080 - val_loss: 1.2129\n",
      "Epoch 53/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0053\n",
      "Epoch 53: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0053 - val_loss: 1.1969\n",
      "Epoch 54/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0048\n",
      "Epoch 54: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0048 - val_loss: 1.2019\n",
      "Epoch 55/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0057\n",
      "Epoch 55: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0057 - val_loss: 1.2345\n",
      "Epoch 56/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0111\n",
      "Epoch 56: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0111 - val_loss: 1.2797\n",
      "Epoch 57/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0142\n",
      "Epoch 57: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0142 - val_loss: 1.2123\n",
      "Epoch 58/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0080\n",
      "Epoch 58: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0080 - val_loss: 1.2174\n",
      "Epoch 59/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0153\n",
      "Epoch 59: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0146 - val_loss: 1.1615\n",
      "Epoch 60/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 60: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0072 - val_loss: 1.1143\n",
      "Epoch 61/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0065\n",
      "Epoch 61: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0061 - val_loss: 1.0569\n",
      "Epoch 62/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 62: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0043 - val_loss: 0.9962\n",
      "Epoch 63/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0045\n",
      "Epoch 63: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0045 - val_loss: 0.9738\n",
      "Epoch 64/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0039\n",
      "Epoch 64: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0039 - val_loss: 0.9834\n",
      "Epoch 65/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0051\n",
      "Epoch 65: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0051 - val_loss: 0.9490\n",
      "Epoch 66/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0054\n",
      "Epoch 66: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0054 - val_loss: 0.9476\n",
      "Epoch 67/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0076\n",
      "Epoch 67: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0072 - val_loss: 0.9712\n",
      "Epoch 68/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0078\n",
      "Epoch 68: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0078 - val_loss: 0.9284\n",
      "Epoch 69/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0093\n",
      "Epoch 69: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0093 - val_loss: 0.9589\n",
      "Epoch 70/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0074\n",
      "Epoch 70: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0079 - val_loss: 0.9560\n",
      "Epoch 71/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0080\n",
      "Epoch 71: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0080 - val_loss: 0.9275\n",
      "Epoch 72/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 72: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 58ms/step - loss: 0.0108 - val_loss: 0.9081\n",
      "Epoch 73/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0080\n",
      "Epoch 73: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0080 - val_loss: 0.8769\n",
      "Epoch 74/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0065\n",
      "Epoch 74: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0065 - val_loss: 0.8903\n",
      "Epoch 75/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0100\n",
      "Epoch 75: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0100 - val_loss: 0.8880\n",
      "Epoch 76/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0132\n",
      "Epoch 76: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0132 - val_loss: 0.8400\n",
      "Epoch 77/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0090\n",
      "Epoch 77: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0083 - val_loss: 0.8233\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0064\n",
      "Epoch 78: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0064 - val_loss: 0.8161\n",
      "Epoch 79/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0054\n",
      "Epoch 79: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0054 - val_loss: 0.8365\n",
      "Epoch 80/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 80: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0050 - val_loss: 0.8079\n",
      "Epoch 81/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0067\n",
      "Epoch 81: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0067 - val_loss: 0.8010\n",
      "Epoch 82/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0087\n",
      "Epoch 82: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0087 - val_loss: 0.8500\n",
      "Epoch 83/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0104\n",
      "Epoch 83: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0104 - val_loss: 0.8321\n",
      "Epoch 84/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0065\n",
      "Epoch 84: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0065 - val_loss: 0.7983\n",
      "Epoch 85/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0047\n",
      "Epoch 85: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0047 - val_loss: 0.7856\n",
      "Epoch 86/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 86: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0059 - val_loss: 0.7915\n",
      "Epoch 87/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0048\n",
      "Epoch 87: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0048 - val_loss: 0.7729\n",
      "Epoch 88/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0066\n",
      "Epoch 88: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0070 - val_loss: 0.7519\n",
      "Epoch 89/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0046\n",
      "Epoch 89: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0046 - val_loss: 0.7363\n",
      "Epoch 90/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0046\n",
      "Epoch 90: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0046 - val_loss: 0.7762\n",
      "Epoch 91/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0035\n",
      "Epoch 91: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0035 - val_loss: 0.7529\n",
      "Epoch 92/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0042\n",
      "Epoch 92: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0044 - val_loss: 0.7599\n",
      "Epoch 93/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 93: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0033 - val_loss: 0.7700\n",
      "Epoch 94/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 94: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0033 - val_loss: 0.7469\n",
      "Epoch 95/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 95: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0050 - val_loss: 0.7376\n",
      "Epoch 96/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0057\n",
      "Epoch 96: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0057 - val_loss: 0.7638\n",
      "Epoch 97/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0061\n",
      "Epoch 97: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0061 - val_loss: 0.7295\n",
      "Epoch 98/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0039\n",
      "Epoch 98: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0044 - val_loss: 0.7146\n",
      "Epoch 99/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0050\n",
      "Epoch 99: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0049 - val_loss: 0.7063\n",
      "Epoch 100/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0049\n",
      "Epoch 100: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0049 - val_loss: 0.7008\n",
      "Epoch 101/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0031\n",
      "Epoch 101: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0031 - val_loss: 0.6803\n",
      "Epoch 102/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0033\n",
      "Epoch 102: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0030 - val_loss: 0.6958\n",
      "Epoch 103/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0047\n",
      "Epoch 103: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0047 - val_loss: 0.6973\n",
      "Epoch 104/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0020\n",
      "Epoch 104: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0027 - val_loss: 0.6836\n",
      "Epoch 105/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0029\n",
      "Epoch 105: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0029 - val_loss: 0.6800\n",
      "Epoch 106/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0035\n",
      "Epoch 106: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0035 - val_loss: 0.7179\n",
      "Epoch 107/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0027\n",
      "Epoch 107: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0024 - val_loss: 0.7243\n",
      "Epoch 108/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0029\n",
      "Epoch 108: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0029 - val_loss: 0.6980\n",
      "Epoch 109/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0014\n",
      "Epoch 109: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0014 - val_loss: 0.7299\n",
      "Epoch 110/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020\n",
      "Epoch 110: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0020 - val_loss: 0.7405\n",
      "Epoch 111/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0024  \n",
      "Epoch 111: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0024 - val_loss: 0.7636\n",
      "Epoch 112/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0018\n",
      "Epoch 112: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0018 - val_loss: 0.7697\n",
      "Epoch 113/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0055\n",
      "Epoch 113: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0055 - val_loss: 0.7873\n",
      "Epoch 114/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0058\n",
      "Epoch 114: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 0.0058 - val_loss: 0.7600\n",
      "Epoch 115/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0042\n",
      "Epoch 115: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0038 - val_loss: 0.8368\n",
      "Epoch 116/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0025\n",
      "Epoch 116: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0025 - val_loss: 0.7855\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0028  \n",
      "Epoch 117: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0028 - val_loss: 0.8183\n",
      "Epoch 118/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 118: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0033 - val_loss: 0.8544\n",
      "Epoch 119/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0055\n",
      "Epoch 119: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0066 - val_loss: 0.7912\n",
      "Epoch 120/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0064\n",
      "Epoch 120: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0056 - val_loss: 0.8440\n",
      "Epoch 121/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 121: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0050 - val_loss: 0.8554\n",
      "Epoch 122/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 122: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0059 - val_loss: 0.8704\n",
      "Epoch 123/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0044\n",
      "Epoch 123: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0044 - val_loss: 0.8728\n",
      "Epoch 124/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0034\n",
      "Epoch 124: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0034 - val_loss: 0.9030\n",
      "Epoch 125/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0031\n",
      "Epoch 125: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0031 - val_loss: 0.9146\n",
      "Epoch 126/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0029\n",
      "Epoch 126: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0029 - val_loss: 0.8862\n",
      "Epoch 127/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0049\n",
      "Epoch 127: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0049 - val_loss: 0.8706\n",
      "Epoch 128/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0056\n",
      "Epoch 128: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0056 - val_loss: 0.8696\n",
      "Epoch 129/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0047\n",
      "Epoch 129: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0047 - val_loss: 0.8133\n",
      "Epoch 130/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0039\n",
      "Epoch 130: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.0039 - val_loss: 0.8739\n",
      "Epoch 131/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0069\n",
      "Epoch 131: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0069 - val_loss: 0.8112\n",
      "Epoch 132/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0068\n",
      "Epoch 132: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0068 - val_loss: 0.7795\n",
      "Epoch 133/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0067\n",
      "Epoch 133: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0067 - val_loss: 0.7578\n",
      "Epoch 134/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 134: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0043 - val_loss: 0.7587\n",
      "Epoch 135/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 135: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0023 - val_loss: 0.7041\n",
      "Epoch 136/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 136: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0021 - val_loss: 0.7417\n",
      "Epoch 137/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0037\n",
      "Epoch 137: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0037 - val_loss: 0.7191\n",
      "Epoch 138/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023  \n",
      "Epoch 138: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0023 - val_loss: 0.6803\n",
      "Epoch 139/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0019\n",
      "Epoch 139: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0018 - val_loss: 0.6846\n",
      "Epoch 140/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0031\n",
      "Epoch 140: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0027 - val_loss: 0.7041\n",
      "Epoch 141/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 141: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0023 - val_loss: 0.6619\n",
      "Epoch 142/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 142: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0023 - val_loss: 0.6714\n",
      "Epoch 143/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013\n",
      "Epoch 143: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0013 - val_loss: 0.6315\n",
      "Epoch 144/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 144: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0024 - val_loss: 0.6012\n",
      "Epoch 145/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0031\n",
      "Epoch 145: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 0.0030 - val_loss: 0.5798\n",
      "Epoch 146/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0048\n",
      "Epoch 146: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0044 - val_loss: 0.5966\n",
      "Epoch 147/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0039\n",
      "Epoch 147: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0039 - val_loss: 0.5712\n",
      "Epoch 148/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 148: val_loss did not improve from 0.56044\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0041 - val_loss: 0.5699\n",
      "Epoch 149/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 149: val_loss improved from 0.56044 to 0.52866, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 0.0033 - val_loss: 0.5287\n",
      "Epoch 150/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0050\n",
      "Epoch 150: val_loss did not improve from 0.52866\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0047 - val_loss: 0.5596\n",
      "Epoch 151/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0045\n",
      "Epoch 151: val_loss improved from 0.52866 to 0.49038, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.0045 - val_loss: 0.4904\n",
      "Epoch 152/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0031\n",
      "Epoch 152: val_loss did not improve from 0.49038\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0031 - val_loss: 0.5054\n",
      "Epoch 153/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 153: val_loss did not improve from 0.49038\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0024 - val_loss: 0.5141\n",
      "Epoch 154/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0025\n",
      "Epoch 154: val_loss improved from 0.49038 to 0.47763, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.0022 - val_loss: 0.4776\n",
      "Epoch 155/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 155: val_loss improved from 0.47763 to 0.45898, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.0043 - val_loss: 0.4590\n",
      "Epoch 156/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0050\n",
      "Epoch 156: val_loss improved from 0.45898 to 0.44965, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 0.0050 - val_loss: 0.4496\n",
      "Epoch 157/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0018\n",
      "Epoch 157: val_loss improved from 0.44965 to 0.42352, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 113ms/step - loss: 0.0019 - val_loss: 0.4235\n",
      "Epoch 158/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019  \n",
      "Epoch 158: val_loss improved from 0.42352 to 0.40832, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.0019 - val_loss: 0.4083\n",
      "Epoch 159/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 159: val_loss improved from 0.40832 to 0.38467, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.0023 - val_loss: 0.3847\n",
      "Epoch 160/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0012    \n",
      "Epoch 160: val_loss improved from 0.38467 to 0.36692, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 0.0013 - val_loss: 0.3669\n",
      "Epoch 161/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0019\n",
      "Epoch 161: val_loss improved from 0.36692 to 0.32222, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.0020 - val_loss: 0.3222\n",
      "Epoch 162/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0021\n",
      "Epoch 162: val_loss did not improve from 0.32222\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 0.0024 - val_loss: 0.3290\n",
      "Epoch 163/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0022\n",
      "Epoch 163: val_loss improved from 0.32222 to 0.29651, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.0022 - val_loss: 0.2965\n",
      "Epoch 164/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0028\n",
      "Epoch 164: val_loss improved from 0.29651 to 0.26674, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 113ms/step - loss: 0.0028 - val_loss: 0.2667\n",
      "Epoch 165/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0039  \n",
      "Epoch 165: val_loss improved from 0.26674 to 0.25865, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 0.0039 - val_loss: 0.2587\n",
      "Epoch 166/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 166: val_loss improved from 0.25865 to 0.23109, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 0.0021 - val_loss: 0.2311\n",
      "Epoch 167/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013\n",
      "Epoch 167: val_loss improved from 0.23109 to 0.22258, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 113ms/step - loss: 0.0013 - val_loss: 0.2226\n",
      "Epoch 168/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017\n",
      "Epoch 168: val_loss improved from 0.22258 to 0.19581, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.0017 - val_loss: 0.1958\n",
      "Epoch 169/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0027\n",
      "Epoch 169: val_loss improved from 0.19581 to 0.16552, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.0027 - val_loss: 0.1655\n",
      "Epoch 170/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0018  \n",
      "Epoch 170: val_loss improved from 0.16552 to 0.13272, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 0.0016 - val_loss: 0.1327\n",
      "Epoch 171/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0030\n",
      "Epoch 171: val_loss did not improve from 0.13272\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0030 - val_loss: 0.1365\n",
      "Epoch 172/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 172: val_loss improved from 0.13272 to 0.11782, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 0.0021 - val_loss: 0.1178\n",
      "Epoch 173/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0016\n",
      "Epoch 173: val_loss improved from 0.11782 to 0.10257, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 119ms/step - loss: 0.0016 - val_loss: 0.1026\n",
      "Epoch 174/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0012    \n",
      "Epoch 174: val_loss improved from 0.10257 to 0.07374, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 113ms/step - loss: 0.0012 - val_loss: 0.0737\n",
      "Epoch 175/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020\n",
      "Epoch 175: val_loss did not improve from 0.07374\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0020 - val_loss: 0.0843\n",
      "Epoch 176/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0038  \n",
      "Epoch 176: val_loss did not improve from 0.07374\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0038 - val_loss: 0.0942\n",
      "Epoch 177/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0034\n",
      "Epoch 177: val_loss improved from 0.07374 to 0.06479, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 0.0034 - val_loss: 0.0648\n",
      "Epoch 178/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0018  \n",
      "Epoch 178: val_loss improved from 0.06479 to 0.06021, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 0.0018 - val_loss: 0.0602\n",
      "Epoch 179/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0030\n",
      "Epoch 179: val_loss improved from 0.06021 to 0.05682, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.0030 - val_loss: 0.0568\n",
      "Epoch 180/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0045\n",
      "Epoch 180: val_loss did not improve from 0.05682\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0045 - val_loss: 0.0584\n",
      "Epoch 181/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0038\n",
      "Epoch 181: val_loss improved from 0.05682 to 0.04555, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.0039 - val_loss: 0.0456\n",
      "Epoch 182/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0044\n",
      "Epoch 182: val_loss improved from 0.04555 to 0.03052, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.0040 - val_loss: 0.0305\n",
      "Epoch 183/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0039\n",
      "Epoch 183: val_loss did not improve from 0.03052\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0039 - val_loss: 0.0518\n",
      "Epoch 184/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 184: val_loss did not improve from 0.03052\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0021 - val_loss: 0.0421\n",
      "Epoch 185/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 185: val_loss did not improve from 0.03052\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0024 - val_loss: 0.0338\n",
      "Epoch 186/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 186: val_loss did not improve from 0.03052\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0024 - val_loss: 0.0482\n",
      "Epoch 187/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0046\n",
      "Epoch 187: val_loss improved from 0.03052 to 0.02259, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 123ms/step - loss: 0.0046 - val_loss: 0.0226\n",
      "Epoch 188/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0068\n",
      "Epoch 188: val_loss improved from 0.02259 to 0.01920, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.0068 - val_loss: 0.0192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0065\n",
      "Epoch 189: val_loss did not improve from 0.01920\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0060 - val_loss: 0.0464\n",
      "Epoch 190/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0055\n",
      "Epoch 190: val_loss did not improve from 0.01920\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0055 - val_loss: 0.0273\n",
      "Epoch 191/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0042\n",
      "Epoch 191: val_loss did not improve from 0.01920\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0042 - val_loss: 0.0251\n",
      "Epoch 192/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0034\n",
      "Epoch 192: val_loss did not improve from 0.01920\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0034 - val_loss: 0.0260\n",
      "Epoch 193/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 193: val_loss improved from 0.01920 to 0.01625, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 0.0040 - val_loss: 0.0163\n",
      "Epoch 194/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0025\n",
      "Epoch 194: val_loss did not improve from 0.01625\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0025 - val_loss: 0.0188\n",
      "Epoch 195/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0043\n",
      "Epoch 195: val_loss did not improve from 0.01625\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0044 - val_loss: 0.0220\n",
      "Epoch 196/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0057\n",
      "Epoch 196: val_loss improved from 0.01625 to 0.01249, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 117ms/step - loss: 0.0056 - val_loss: 0.0125\n",
      "Epoch 197/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0036\n",
      "Epoch 197: val_loss did not improve from 0.01249\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0036 - val_loss: 0.0194\n",
      "Epoch 198/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017\n",
      "Epoch 198: val_loss did not improve from 0.01249\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0017 - val_loss: 0.0245\n",
      "Epoch 199/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017\n",
      "Epoch 199: val_loss did not improve from 0.01249\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0017 - val_loss: 0.0214\n",
      "Epoch 200/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017\n",
      "Epoch 200: val_loss did not improve from 0.01249\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0017 - val_loss: 0.0169\n",
      "Epoch 201/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019\n",
      "Epoch 201: val_loss did not improve from 0.01249\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0019 - val_loss: 0.0226\n",
      "Epoch 202/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 202: val_loss did not improve from 0.01249\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0023 - val_loss: 0.0169\n",
      "Epoch 203/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020\n",
      "Epoch 203: val_loss improved from 0.01249 to 0.00920, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 0.0020 - val_loss: 0.0092\n",
      "Epoch 204/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 204: val_loss did not improve from 0.00920\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0024 - val_loss: 0.0194\n",
      "Epoch 205/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0030\n",
      "Epoch 205: val_loss did not improve from 0.00920\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0030 - val_loss: 0.0123\n",
      "Epoch 206/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017  \n",
      "Epoch 206: val_loss did not improve from 0.00920\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0017 - val_loss: 0.0098\n",
      "Epoch 207/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013\n",
      "Epoch 207: val_loss did not improve from 0.00920\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0013 - val_loss: 0.0171\n",
      "Epoch 208/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019\n",
      "Epoch 208: val_loss did not improve from 0.00920\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0019 - val_loss: 0.0103\n",
      "Epoch 209/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020  \n",
      "Epoch 209: val_loss did not improve from 0.00920\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0020 - val_loss: 0.0168\n",
      "Epoch 210/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0028\n",
      "Epoch 210: val_loss did not improve from 0.00920\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0028 - val_loss: 0.0127\n",
      "Epoch 211/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0032\n",
      "Epoch 211: val_loss improved from 0.00920 to 0.00759, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 122ms/step - loss: 0.0032 - val_loss: 0.0076\n",
      "Epoch 212/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0015   \n",
      "Epoch 212: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0015 - val_loss: 0.0124\n",
      "Epoch 213/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0014\n",
      "Epoch 213: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0014 - val_loss: 0.0086\n",
      "Epoch 214/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0012  \n",
      "Epoch 214: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0012 - val_loss: 0.0113\n",
      "Epoch 215/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0026\n",
      "Epoch 215: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0024 - val_loss: 0.0119\n",
      "Epoch 216/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019\n",
      "Epoch 216: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0019 - val_loss: 0.0144\n",
      "Epoch 217/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0018\n",
      "Epoch 217: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0018 - val_loss: 0.0128\n",
      "Epoch 218/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 8.3329e-04\n",
      "Epoch 218: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 8.3329e-04 - val_loss: 0.0119\n",
      "Epoch 219/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0011   \n",
      "Epoch 219: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0011 - val_loss: 0.0123\n",
      "Epoch 220/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.9888e-04\n",
      "Epoch 220: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 9.9888e-04 - val_loss: 0.0129\n",
      "Epoch 221/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0016   \n",
      "Epoch 221: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0016 - val_loss: 0.0119\n",
      "Epoch 222/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0011\n",
      "Epoch 222: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0011 - val_loss: 0.0098\n",
      "Epoch 223/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0011  \n",
      "Epoch 223: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0011 - val_loss: 0.0139\n",
      "Epoch 224/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013\n",
      "Epoch 224: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0013 - val_loss: 0.0153\n",
      "Epoch 225/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0022\n",
      "Epoch 225: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0022 - val_loss: 0.0140\n",
      "Epoch 226/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.7532e-04\n",
      "Epoch 226: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 9.7532e-04 - val_loss: 0.0154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013  \n",
      "Epoch 227: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0013 - val_loss: 0.0105\n",
      "Epoch 228/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0011  \n",
      "Epoch 228: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.0011 - val_loss: 0.0137\n",
      "Epoch 229/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.3213e-04\n",
      "Epoch 229: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 7.3213e-04 - val_loss: 0.0114\n",
      "Epoch 230/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 8.0041e-04\n",
      "Epoch 230: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 8.0041e-04 - val_loss: 0.0132\n",
      "Epoch 231/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.8591e-04\n",
      "Epoch 231: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 9.8591e-04 - val_loss: 0.0131\n",
      "Epoch 232/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0015\n",
      "Epoch 232: val_loss did not improve from 0.00759\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0015 - val_loss: 0.0097\n",
      "Epoch 233/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0011  \n",
      "Epoch 233: val_loss improved from 0.00759 to 0.00635, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 125ms/step - loss: 0.0011 - val_loss: 0.0064\n",
      "Epoch 234/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0015\n",
      "Epoch 234: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0017 - val_loss: 0.0133\n",
      "Epoch 235/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0016\n",
      "Epoch 235: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0016 - val_loss: 0.0081\n",
      "Epoch 236/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0015  \n",
      "Epoch 236: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0015 - val_loss: 0.0116\n",
      "Epoch 237/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.3937e-04\n",
      "Epoch 237: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 9.3937e-04 - val_loss: 0.0143\n",
      "Epoch 238/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0020\n",
      "Epoch 238: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0020 - val_loss: 0.0154\n",
      "Epoch 239/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017\n",
      "Epoch 239: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0017 - val_loss: 0.0101\n",
      "Epoch 240/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013   \n",
      "Epoch 240: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0013 - val_loss: 0.0107\n",
      "Epoch 241/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0018  \n",
      "Epoch 241: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0018 - val_loss: 0.0106\n",
      "Epoch 242/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 242: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0021 - val_loss: 0.0135\n",
      "Epoch 243/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0016  \n",
      "Epoch 243: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0016 - val_loss: 0.0138\n",
      "Epoch 244/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0020\n",
      "Epoch 244: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0018 - val_loss: 0.0150\n",
      "Epoch 245/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0019\n",
      "Epoch 245: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 57ms/step - loss: 0.0020 - val_loss: 0.0156\n",
      "Epoch 246/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0027\n",
      "Epoch 246: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0025 - val_loss: 0.0125\n",
      "Epoch 247/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0026\n",
      "Epoch 247: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0026 - val_loss: 0.0133\n",
      "Epoch 248/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 248: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0033 - val_loss: 0.0161\n",
      "Epoch 249/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0030\n",
      "Epoch 249: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0030 - val_loss: 0.0167\n",
      "Epoch 250/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 250: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0033 - val_loss: 0.0113\n",
      "Epoch 251/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0016\n",
      "Epoch 251: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0016 - val_loss: 0.0120\n",
      "Epoch 252/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0010   \n",
      "Epoch 252: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0010 - val_loss: 0.0127\n",
      "Epoch 253/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 253: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0021 - val_loss: 0.0163\n",
      "Epoch 254/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0029\n",
      "Epoch 254: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0028 - val_loss: 0.0194\n",
      "Epoch 255/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0053\n",
      "Epoch 255: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0053 - val_loss: 0.0222\n",
      "Epoch 256/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0036\n",
      "Epoch 256: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0036 - val_loss: 0.0122\n",
      "Epoch 257/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0031\n",
      "Epoch 257: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0031 - val_loss: 0.0148\n",
      "Epoch 258/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0031\n",
      "Epoch 258: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0031 - val_loss: 0.0096\n",
      "Epoch 259/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0038\n",
      "Epoch 259: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0038 - val_loss: 0.0192\n",
      "Epoch 260/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 260: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0043 - val_loss: 0.0170\n",
      "Epoch 261/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020\n",
      "Epoch 261: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0020 - val_loss: 0.0180\n",
      "Epoch 262/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019\n",
      "Epoch 262: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0019 - val_loss: 0.0106\n",
      "Epoch 263/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0032\n",
      "Epoch 263: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0032 - val_loss: 0.0178\n",
      "Epoch 264/300\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 0.0020\n",
      "Epoch 264: val_loss did not improve from 0.00635\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0018 - val_loss: 0.0212\n",
      "Epoch 265/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0034\n",
      "Epoch 265: val_loss did not improve from 0.00635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0034 - val_loss: 0.0132\n",
      "Epoch 266/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023  \n",
      "Epoch 266: val_loss improved from 0.00635 to 0.00553, saving model to model_weights_best.h5\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.0023 - val_loss: 0.0055\n",
      "Epoch 267/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0026\n",
      "Epoch 267: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0026 - val_loss: 0.0096\n",
      "Epoch 268/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0013  \n",
      "Epoch 268: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0013 - val_loss: 0.0130\n",
      "Epoch 269/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.5700e-04\n",
      "Epoch 269: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 9.5700e-04 - val_loss: 0.0074\n",
      "Epoch 270/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0011  \n",
      "Epoch 270: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0011 - val_loss: 0.0102\n",
      "Epoch 271/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0010\n",
      "Epoch 271: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0011 - val_loss: 0.0107\n",
      "Epoch 272/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0016  \n",
      "Epoch 272: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 56ms/step - loss: 0.0016 - val_loss: 0.0166\n",
      "Epoch 273/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 273: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0023 - val_loss: 0.0098\n",
      "Epoch 274/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0011\n",
      "Epoch 274: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0010 - val_loss: 0.0114\n",
      "Epoch 275/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0011\n",
      "Epoch 275: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0011 - val_loss: 0.0114\n",
      "Epoch 276/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.1735e-04\n",
      "Epoch 276: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 9.1735e-04 - val_loss: 0.0111\n",
      "Epoch 277/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0021 \n",
      "Epoch 277: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.0021 - val_loss: 0.0194\n",
      "Epoch 278/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0025\n",
      "Epoch 278: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0025 - val_loss: 0.0120\n",
      "Epoch 279/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020\n",
      "Epoch 279: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0020 - val_loss: 0.0097\n",
      "Epoch 280/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019  \n",
      "Epoch 280: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0019 - val_loss: 0.0139\n",
      "Epoch 281/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0023  \n",
      "Epoch 281: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0023 - val_loss: 0.0129\n",
      "Epoch 282/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020  \n",
      "Epoch 282: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0020 - val_loss: 0.0126\n",
      "Epoch 283/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017\n",
      "Epoch 283: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0017 - val_loss: 0.0130\n",
      "Epoch 284/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.4131e-04\n",
      "Epoch 284: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 9.4131e-04 - val_loss: 0.0100\n",
      "Epoch 285/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 285: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0024 - val_loss: 0.0168\n",
      "Epoch 286/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0017\n",
      "Epoch 286: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0019 - val_loss: 0.0115\n",
      "Epoch 287/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 287: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0023 - val_loss: 0.0099\n",
      "Epoch 288/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 9.7775e-04\n",
      "Epoch 288: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 9.7775e-04 - val_loss: 0.0094\n",
      "Epoch 289/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0016\n",
      "Epoch 289: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.0016 - val_loss: 0.0132\n",
      "Epoch 290/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0015\n",
      "Epoch 290: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.0015 - val_loss: 0.0101\n",
      "Epoch 291/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0023\n",
      "Epoch 291: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 51ms/step - loss: 0.0023 - val_loss: 0.0133\n",
      "Epoch 292/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0018\n",
      "Epoch 292: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0018 - val_loss: 0.0105\n",
      "Epoch 293/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0015\n",
      "Epoch 293: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0015 - val_loss: 0.0098\n",
      "Epoch 294/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0014  \n",
      "Epoch 294: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 52ms/step - loss: 0.0014 - val_loss: 0.0131\n",
      "Epoch 295/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019  \n",
      "Epoch 295: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 47ms/step - loss: 0.0019 - val_loss: 0.0121\n",
      "Epoch 296/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0015\n",
      "Epoch 296: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.0015 - val_loss: 0.0096\n",
      "Epoch 297/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0019\n",
      "Epoch 297: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0019 - val_loss: 0.0167\n",
      "Epoch 298/300\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0018\n",
      "Epoch 298: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 50ms/step - loss: 0.0017 - val_loss: 0.0063\n",
      "Epoch 299/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0012\n",
      "Epoch 299: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0012 - val_loss: 0.0072\n",
      "Epoch 300/300\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0012   \n",
      "Epoch 300: val_loss did not improve from 0.00553\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0012 - val_loss: 0.0126\n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(\n",
    "    log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False, update_freq='epoch'\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model_weights_best.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=300,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    callbacks=[checkpoint, tensorboard],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb68d5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fcff8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57057baf",
   "metadata": {},
   "source": [
    "### Use the model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f441a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "The possibility is 10.511856079101562\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image_path = '/home/jeffreydhy/data/projects/ares-finance/generated/images/unlabeled/AMD_30min_segments/segment_2023-04-21_53.png'\n",
    "test_image = cv2.imread(test_image_path)\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Preprocess the test image\n",
    "input_shape = (224, 224, 3)\n",
    "resized_image = cv2.resize(test_image, input_shape[:2])\n",
    "preprocessed_image = resized_image / 255.0\n",
    "preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(preprocessed_image)[0]\n",
    "\n",
    "# Post-process the prediction\n",
    "xmin, ymin, xmax, ymax, prob = prediction\n",
    "print(\"The possibility is {}\".format(prob))\n",
    "xmin = int(xmin * test_image.shape[1])\n",
    "ymin = int(ymin * test_image.shape[0])\n",
    "xmax = int(xmax * test_image.shape[1])\n",
    "ymax = int(ymax * test_image.shape[0])\n",
    "\n",
    "# Visualize the results\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(test_image)\n",
    "rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eadb25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ares",
   "language": "python",
   "name": "ares"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
